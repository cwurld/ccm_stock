{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trading Strategy Based on Leading Indicators\n",
    "\n",
    "I don't know much about stock trading strategies. The reason I created this notebook is to play around with Python data science tools and the stock market has lots of data. Also I wanted something \"actionable\" if the idea showed promise.\n",
    "\n",
    "One of the reasons I have not looked into stock trading strategies is I believe in the [\"Efficient Market Hypothesis (EMH)\"](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp). This basically says that you can't make money off information that is known to everyone because if everyone uses that information, that will cause the price to change and erase the gains. By the time Jim Cramer is talking about it, you are too late. Or worse you are about to be the victim of a pump-and-dump operation.\n",
    "\n",
    "Then again, as a programmer, I have always been fascinated by the [\"combinatorial explosion\"](https://en.wikipedia.org/wiki/Combinatorial_explosion); as the number of things available to combine grows, the number of possible combinations grows exponentially. In the stock market, its plausible that some combination of financial measures could predict short-term changes in a given stock's price. Since there are so many possible combinations of financial measures, it seems possible to find one that no one else knows about; thus avoiding the EMH.\n",
    "\n",
    "Of course, when searching such a large space of possible predictors, it is likely that many of these will be randomly correlated with the target stock. When that is the case, future predictions will be essentially random. Which means the returns will be approximately 0, minus the trading costs. For now, we will assume the trading costs are 0, so the costs of random correlations will just be the opportunity cost of not investing in a better performing strategy.\n",
    "\n",
    "## The Strategy\n",
    "The goal is to predict short-term price spikes in a target stock based on patterns in other finacial measures. To do this, we will:\n",
    "\n",
    "1. pick a target stock\n",
    "2. find all the price spikes in the recent historical data of that stock\n",
    "3. look for patterns in the short-term historical data of many other financial measures, right before the spikes in the target\n",
    "4. combine them to make a predictor for that stock\n",
    "\n",
    "## The Financial Measures\n",
    "Their are many possible financial measures; gold, oil, sentiment analysis of Reddit, ... To make things easy for now, we will just use historical stock data that can be obtained from Yahoo, for free. More specifically, the NASDAQ.\n",
    "\n",
    "## Spikes\n",
    "We want the spikes to be something we can make money off of. It seems like it will be easier to predict short-term changes. We will try this pattern: look for a sequence of 3 prices that are monotonically increasing and have a gain that is greater than 3% (e.g. p[t+2] > p[t+1] > p[t] and (p[t+2] - p[t])/p[t] > 0.03).\n",
    "\n",
    "## The Power of Compounding\n",
    "A 3% gain on a trade does not seem very exciting. But what if we could find one trade per week with a 3% gain? That would give a 465% annual return. Of course that is easier said than done. And there certainly would be trades that lost money. The point being that 3% is not an insignificant target. \n",
    "\n",
    "## Estimating the Predictor Function\n",
    "The first step is to de-trend the data by converting the predictor stock prices into fractional changes: (p[t+1] - p[t])/p[t]. If we do not de-trend the data, the predictor functions end up being the trend line and not be correlated with the spikes.\n",
    "\n",
    "Before each spike, we assume the fractional changes will be the predictor function plus random noise. We reduce the noise by signal averaging the changes from all spikes.\n",
    "\n",
    "## The Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7611 entries, 0 to 7610\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Symbol      7611 non-null   object \n",
      " 1   Name        7611 non-null   object \n",
      " 2   Last Sale   7611 non-null   object \n",
      " 3   Net Change  7611 non-null   float64\n",
      " 4   % Change    7598 non-null   object \n",
      " 5   Market Cap  6987 non-null   float64\n",
      " 6   Country     6951 non-null   object \n",
      " 7   IPO Year    4341 non-null   float64\n",
      " 8   Volume      7611 non-null   int64  \n",
      " 9   Sector      5502 non-null   object \n",
      " 10  Industry    5501 non-null   object \n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 654.2+ KB\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import leading_pattern.model as model\n",
    "import leading_pattern.by_yield as by_yield\n",
    "import leading_pattern.roc_auc as roc_auc\n",
    "import leading_pattern.plotting as plotting\n",
    "import my_utils.loaders as loaders\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "np.seterr(all='raise')  # make code stop on warnings\n",
    "\n",
    "# Two years of historical data. The first year is for training, the second year is for testing.\n",
    "CONFIG = {\n",
    "    'start_date': '20170103',\n",
    "    'split_date': '20171231',  # We do 2 splits. Before this date is training data, after is test data\n",
    "    'end_date': '20181231',\n",
    "    'spike_length': 2,  # in trading days\n",
    "    'spike_threshold_percent': 3.0,\n",
    "    'predictor_func_length': 5,  # in trading days\n",
    "    'use_diffs': True,\n",
    "    'price_field': 'Open',\n",
    "    'predictor_field': 'Open'\n",
    "}\n",
    "\n",
    "# We can get a CSV file of all the stocks in the NASDAQ from here: https://www.nasdaq.com/market-activity/stocks/screener\n",
    "# Lets read all the stocks into a pandas dataframe\n",
    "all_stocks = model.read_nasdaq_stock_list()\n",
    "all_stocks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any data set like this, there are bound to be some oddballs. Lets look at some summary statistics.\n",
    "\n",
    "But first lets fix \"Last Sale\", which is the price of the last sale. The values have a leading $, so pandas treats it as a string. Lets convert it to a float. Lets also add a column that is the \"Last Sale\" x \"Volume\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last Sale</th>\n",
       "      <th>Net Change</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>IPO Year</th>\n",
       "      <th>Volume</th>\n",
       "      <th>dollar_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7611.000000</td>\n",
       "      <td>7611.000000</td>\n",
       "      <td>6.987000e+03</td>\n",
       "      <td>4341.000000</td>\n",
       "      <td>7.611000e+03</td>\n",
       "      <td>7.611000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>97.379191</td>\n",
       "      <td>0.938466</td>\n",
       "      <td>8.862684e+09</td>\n",
       "      <td>2013.480534</td>\n",
       "      <td>1.039174e+06</td>\n",
       "      <td>4.744837e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4687.640393</td>\n",
       "      <td>33.756277</td>\n",
       "      <td>5.822363e+10</td>\n",
       "      <td>9.247937</td>\n",
       "      <td>7.483577e+06</td>\n",
       "      <td>3.623841e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.052000</td>\n",
       "      <td>-11.470000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.250000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.475000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.752196e+07</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2.209850e+04</td>\n",
       "      <td>2.550942e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.240000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>5.066325e+08</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>1.299790e+05</td>\n",
       "      <td>1.733926e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.450000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>3.174283e+09</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>5.591820e+05</td>\n",
       "      <td>1.470607e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>408840.000000</td>\n",
       "      <td>2939.000000</td>\n",
       "      <td>2.328752e+12</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>4.955231e+08</td>\n",
       "      <td>2.069273e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Last Sale   Net Change    Market Cap     IPO Year        Volume  \\\n",
       "count    7611.000000  7611.000000  6.987000e+03  4341.000000  7.611000e+03   \n",
       "mean       97.379191     0.938466  8.862684e+09  2013.480534  1.039174e+06   \n",
       "std      4687.640393    33.756277  5.822363e+10     9.247937  7.483577e+06   \n",
       "min         0.052000   -11.470000  0.000000e+00  1972.000000  1.000000e+00   \n",
       "25%         9.475000     0.000000  9.752196e+07  2011.000000  2.209850e+04   \n",
       "50%        18.240000     0.090000  5.066325e+08  2017.000000  1.299790e+05   \n",
       "75%        36.450000     0.480000  3.174283e+09  2020.000000  5.591820e+05   \n",
       "max    408840.000000  2939.000000  2.328752e+12  2021.000000  4.955231e+08   \n",
       "\n",
       "         dollar_vol  \n",
       "count  7.611000e+03  \n",
       "mean   4.744837e+07  \n",
       "std    3.623841e+08  \n",
       "min    4.250000e-01  \n",
       "25%    2.550942e+05  \n",
       "50%    1.733926e+06  \n",
       "75%    1.470607e+07  \n",
       "max    2.069273e+10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stocks['Last Sale'] = all_stocks['Last Sale'].apply(lambda x: float(x.replace('$','')))\n",
    "all_stocks['dollar_vol'] = all_stocks['Last Sale']* all_stocks['Volume']\n",
    "all_stocks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last-Sale is the price of the last trade. Look at that max! Almost a half million. IPO Year is a little odd because Yahoo reader has convert the int into a float. We will exclude stocks that had an IPO Year after our analysis start year minus one year.\n",
    "\n",
    "Volume seems like might be a reasonable field for removing outliers. To speed up the exploratory analysis, lets limit the number of predictor stocks to a random sample of 10, whose volume is above the 0.1 quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 251 entries, 2017-01-03 to 2017-12-29\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   High       251 non-null    float64\n",
      " 1   Low        251 non-null    float64\n",
      " 2   Open       251 non-null    float64\n",
      " 3   Close      251 non-null    float64\n",
      " 4   Volume     251 non-null    float64\n",
      " 5   Adj Close  251 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "analysis_start_year = float(CONFIG['start_date'][0:4])\n",
    "symbols = model.select_random_stock_symbols(all_stocks, 10, analysis_start_year, quantile=0.1, filter_field='Volume')\n",
    "\n",
    "# Use these from a previous random sample\n",
    "symbols = ['NPTN', 'MTL', 'AGR', 'PHD', 'ESS', 'CBOE', 'AGRO', 'FPF', 'CHMA', 'SEAS']\n",
    "\n",
    "data_sets = loaders.get_data_sets(symbols, CONFIG)\n",
    "\n",
    "# Lets select the target stock and look and it's columns\n",
    "target_stock = symbols[0]  # we will try to predict the first one\n",
    "print(data_sets['train'][target_stock].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear which of these columns would be the best predictors. One thought might be to use them all? But there is a problem...\n",
    "\n",
    "## Water Everywhere, Nothing to Drink\n",
    "Although we have lots of data, we do not have enough of the right kind of data to use every tool in the machine learning toolbox. The big problem is that the stock market and the economy are constantly changing. It's not clear if the patterns in the data from 5 years ago are still relevant data now.\n",
    "\n",
    "For now, we are using historical data from one year to predict the next. A typical target stock will have about 20 spikes in a year. Coming up with a predictor based on some combination of all these columns with only 20 data points would be impossible.\n",
    "\n",
    "Also, we chose to limit our analysis to the DAILY opening price. While we could get more data by trying to get hourly data, that was not readily available. Moreover, when the time interval between data points is small enough we might run into other problems. Stock prices are set by trades and trades are discrete causing additional noise. And for a somewhat smooth continuous function, nearby values are highly correlated.\n",
    "\n",
    "## Back to the Historical Stock Data\n",
    "Lets look at the spikes in the target stock in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45343e74c6f24090b5d51af970ff714f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_sets['train'][target_stock]\n",
    "model.find_spikes(df, CONFIG)  # Find spikes in target stock and a column 'spikes' to dataframe.\n",
    "plotting.plot_spikes(df, CONFIG['price_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With 28 spikes, it looks like this target stock has enough volatility to train on. Lets load snippets of historical data in each predictor stock before each spike in the target stock. Note, we are also using the target stock as a predictor of its self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c285a05e59c4924acdde069f1b63d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor_func_by_stock = model.get_predictor_func_by_stock(data_sets['train'], target_stock, symbols, CONFIG)\n",
    "plotting.plot_predictor_functions(symbols, target_stock, predictor_func_by_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Predictor Function\n",
    "To use a predictor function, we do a linear regression on between it and a corresponding length of the predictor stock time series at each point in time. If $r^2$ at a timepoint exceeds a threshold, then we predict that there is a spike in the target. \n",
    "\n",
    "## Calculating Performance of Each Predictor\n",
    "The objective of this strategy is not neccessarily to make correct predictions. Rather it is to make money. Lets try running the predictors and look at the price changes after each positive prediction (true positive and false positive) as a function of threshold of $r^2$\n",
    "\n",
    "One way to do this is to just add up all the price changes to get the total yield. The problem with that approach is it combines the change from each trade with the number of trades. To remove the effect of trading frequency, we will calculate the expected-mean-percent-change.\n",
    "\n",
    "To calculate expected-mean-percent-change, we start by calculating all the percent changes in the test data. If the predictor did not predict the price then it would be like randomly sampling from these changes (null hypothesis). Next we set the $r^2$ threshold to predict when to buy which creates another sample of the percent changes. We use the Kolmogorov-Smirnov to calculate the probability (p) that our sample is the same as the random sample. Then we calculate expected mean percent change (mpc):\n",
    "\n",
    "$expected\\ mpc = p * mpc_{null}+(1-p)*mpc_{predictor}$\n",
    "\n",
    "$\\sigma_{empc}=\\sqrt{p^2*\\sigma_{null}^2+(1-p)^2*\\sigma_{predictor}^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1df1f5cff4a5a82d87a167216474c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_by_predictor = by_yield.calc_performance_by_threshold_for_predictors(target_stock, symbols, CONFIG, data_set_name='test')\n",
    "plotting.plot_calc_performance_scores_by_threshold_for_predictors(target_stock, df_by_predictor, CONFIG['predictor_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot low $r^2$ thresholds to show effectively random predictions. If a predictor worked, we would expect the % changes above 0.5 to look signifcantly better than below 0.5.\n",
    "\n",
    "None of these predictors look great. This is not too surprising; most stocks are not strong predictors of other stocks, much less 10 randomly selected stocks. Further, the target stock itself might not be predictable using this strategy.\n",
    "\n",
    "Just for fun, we can run the code using a different data frame column without having to change the code. Lets try \"Volume\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674fc0eeb6a4418bbf8eb1722f443f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONFIG['predictor_field'] = 'Volume'\n",
    "df_by_predictor = by_yield.calc_performance_by_threshold_for_predictors(target_stock, symbols, CONFIG, data_set_name='test')\n",
    "plotting.plot_calc_performance_scores_by_threshold_for_predictors(target_stock, df_by_predictor, CONFIG['predictor_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only calculate performance for $r^2$ thresholds where there are 5 or more predictions. That is why the threshold range is is truncated for some predictors.\n",
    "\n",
    "The only predictor that might be worth looking into is MTL.\n",
    "\n",
    "## Picking Predictors and Thresholds\n",
    "To simplify the problem we will just look for one predictor and one threshold. But even that is a challenge. Because we have limited data, we only have a training set and a test set. The test set is for simulating how well a predictor would do if we used it for stock trading. For a realistic simulation, we must select the predictor and threshold based on the training data alone.\n",
    "\n",
    "The problem is that some of the good predictors in training are only good due to random correlations. To see how this plays out, we ran the analysis on all stocks with NPTN as the target and then selected the 10 best performers on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predictor      train      test\n",
      "0       NLOK  11.592130  0.020632\n",
      "1       PATK  11.134891  2.982296\n",
      "2       SLNO  10.975609  0.547020\n",
      "3       BIMI  10.246582  0.873495\n",
      "4        LCI  10.036559 -0.091898\n",
      "5       AGNC  10.004727  0.397897\n",
      "6       SPNS   9.921449  1.048367\n",
      "7       CCEP   9.914787  0.639683\n",
      "8        PGC   9.406757  1.599395\n",
      "9        ORA   8.831343  2.030144\n",
      "10       AXS   8.787117 -0.368236\n"
     ]
    }
   ],
   "source": [
    "CONFIG['predictor_field'] = 'Open'\n",
    "df = by_yield.compare_train_test(target_stock, CONFIG)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently the training data is not a very good predictor of the test data. Maybe we need a dev set. \n",
    "\n",
    "## ROC\n",
    "Up to now, our pragmatic use of mean-percent-change combines the predictor performance with the target stock prices and the number of spikes. \n",
    "\n",
    "To look at the predictors in isolation, we can look at the [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves, by predictor, using a range 0f thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9d67e3e5604d9db6fa21bc9783ca2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_set_name = 'test'\n",
    "predictor_func_by_stock = model.get_predictor_func_by_stock(data_sets['train'], target_stock, symbols, CONFIG)\n",
    "auc_df_by_predictor = {}\n",
    "for predictor, p_func in predictor_func_by_stock.items():\n",
    "    model.calc_r_squared_for_list(data_sets[data_set_name], predictor_func_by_stock, CONFIG)\n",
    "    auc_df_by_predictor[predictor] = \\\n",
    "        roc_auc.calc_fpr_tpr(data_sets[data_set_name][target_stock], data_sets[data_set_name][predictor], CONFIG)\n",
    "plotting.plot_roc_panel(auc_df_by_predictor, target_stock, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these look much better than chance.\n",
    "\n",
    "We ran this analysis for all predictors and sorted by the area-under-the-curve (AUC) for the training data. Once again, the training performance is not a good predictor of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  predictor     train      test\n",
      "0       PEI  0.633284  0.512862\n",
      "1      ALJJ  0.629863  0.515574\n",
      "2      AMOT  0.623851  0.501561\n",
      "3       UUU  0.617693  0.546721\n",
      "4       GLP  0.617644  0.519231\n",
      "5       HMY  0.614125  0.516806\n",
      "6      LMFA  0.613978  0.508670\n",
      "7      USDP  0.612512  0.522559\n",
      "8      NOMD  0.611877  0.558720\n",
      "9       CVU  0.610411  0.527120\n"
     ]
    }
   ],
   "source": [
    "df = roc_auc.summarize_auc(target_stock, CONFIG, max_rows=10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like NOMD is the best of the not-very-good. Lets say we somehow selected that as a predictor. How might it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   threshold       fpr       tpr       ppv       npv\n",
      "0       0.95  0.005882  0.000000  0.000000  0.698347\n",
      "1       0.85  0.035294  0.068493  0.454545  0.706897\n",
      "2       0.75  0.052941  0.095890  0.437500  0.709251\n",
      "3       0.65  0.058824  0.109589  0.444444  0.711111\n",
      "4       0.55  0.123529  0.178082  0.382353  0.712919\n",
      "5       0.45  0.217647  0.287671  0.362069  0.718919\n",
      "6       0.35  0.282353  0.410959  0.384615  0.739394\n",
      "7       0.25  0.370588  0.479452  0.357143  0.737931\n",
      "8       0.15  0.458824  0.589041  0.355372  0.754098\n"
     ]
    }
   ],
   "source": [
    "predictor = 'NOMD'\n",
    "data_sets2 = loaders.get_data_sets([target_stock, predictor], CONFIG)\n",
    "predictor_func_by_stock2 = model.get_predictor_func_by_stock(data_sets2['train'], target_stock, [predictor], CONFIG)\n",
    "model.calc_r_squared_for_list(data_sets2['test'], predictor_func_by_stock2, CONFIG)\n",
    "auc_df = roc_auc.calc_fpr_tpr(data_sets2['test'][target_stock], data_sets2['test'][predictor], CONFIG)\n",
    "print(auc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear what threshold is optimal. For sake of argument, lets say 0.65 is best and that tpr=11% with fpr=6%. For NPTN, there a about 30 real spikes in a year. That means we would get 3 true positives and 1.8 false positives. Note, if we select the next higher threshold, the fpr is about twice the tpr and the tpr drops dramatically. Selecting a threshold is clearly important.\n",
    "\n",
    "To double our money, we need to have about 25 true positives in a year. So we would need about 8 stocks like NPTN. That might be possible if we could find a robust method for selecting predictors and thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If we wanted to continue with this strategy, then next step would be to find a way to pick the predictors and thresholds using the shortest time span possible. This could involve including a split just for that. Maybe 10 months for training, 3 months for dev and 3 months for test. Maybe interleave the training and dev splits.\n",
    "\n",
    "Another option would be to look at how the predictor function converges. The predictor function is made by averaging many snippets. If many of the snippets looked similar, then the average might be meaningful. Whereas if each snippet looked completely different, we would still get an average, but it would just be an average of noise.\n",
    "\n",
    "Or maybe a completely different strategy..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccm_stock",
   "language": "python",
   "name": "ccm_stock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}