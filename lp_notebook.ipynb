{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trading Strategy Based on Leading Indicators\n",
    "\n",
    "I don't know much about stock trading strategies. The reason I created this notebook is to play around with Python data science tools and the stock market has lots of data. Also I wanted something \"actionable\" if the idea showed promise.\n",
    "\n",
    "One of the reasons I have not looked into stock trading strategies is I believe in the [\"Efficient Market Hypothesis (EMH)\"](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp). This basically says that you can't make money off well known trading strategies because everyone uses them, which causes the price to change, and erases the potential gains. By the time Jim Cramer is talking about it, you are too late. Or worse you are about to be the victim of a pump-and-dump operation.\n",
    "\n",
    "Then again, as a programmer, I have always been fascinated by the [\"combinatorial explosion\"](https://en.wikipedia.org/wiki/Combinatorial_explosion); as the number of things there is to combine grows, the number of possible combinations grows exponentially. In the stock market, its plausible that some combination of financial measures could predict short-term changes in a given stock's price. Since there are so many possible combinations of financial measures, it seems possible to find one that no one else knows about; thus avoiding the EMH.\n",
    "\n",
    "Of course, when searching such a large space of possible measures, it is likely that many of these will be actually be the results of over-fitting noise. In those cases, future stock prices will not be correlated with the predictors. On average, the return of the strategy will be 0, minus the trading costs. For now, we will assume the trading costs are 0, so the costs of over-fitting will just be the opportunity cost of not investing in a better performing strategy.\n",
    "\n",
    "## The Strategy\n",
    "The goal is to predict short-term price spikes in a target stock based on patterns in other finacial measures. To do this, we will:\n",
    "\n",
    "1. pick a target stock\n",
    "2. find all the price spikes in the recent historical data of that stock\n",
    "3. look for patterns in the short-term historical data of many other financial measures, right before the spikes in the target\n",
    "4. combine them to make a predictor for that stock\n",
    "\n",
    "## The Financial Measures\n",
    "Their are many possible financial measures; gold, oil, sentiment analysis of Reddit, ... To make things easy for now, we will just use historical stock data that can be obtained from Yahoo, for free. More specifically, the NASDAQ.\n",
    "\n",
    "## Spikes\n",
    "We want the spikes to be something we can make money off of. It seems like it will be easier to predict short-term changes. We will try this pattern: look for a sequence of 3 prices that are monotonically increasing and have a gain that is greater than 3% (e.g. p[t+2] > p[t+1] > p[t] and (p[t+2] - p[t])/p[t] > 0.03).\n",
    "\n",
    "## Estimating the Predictor Function\n",
    "We will assume that in some short time window, immediately before each spike<sub>i</sub>, the historical data for a single stock will be:\n",
    "\n",
    "$h_{i}(t) = f(t)(a_{i}p(t) + r(t))$\n",
    "\n",
    "where f(t) is the signal in the predictor stock that would have happened if the economic event that caused the spike in the target did not happen, p(t) is the predictor function for that stock, a<sub>i</sub> is a scalar and r(t) is random noise. \n",
    "\n",
    "Extracting p(t) is only possible with some simplifying assumptions. The perennial favorite is make f(t) piecewise linear by doing a Taylor series expansion. But even linear is kind of messy. \n",
    "\n",
    "In the year after (covid) April 2020, there was an almost unprecedented bull market. During that time, the S&P 500 gained about 12%, giving a weekly gain of about 0.2%. Looks like we can ignore the linear term. Lets just say that f(t) is a constant before each spike.\n",
    "\n",
    "$h_{i}(t) = b_{i}(a_{i}p(t) + r(t))$\n",
    "\n",
    "Averaging $h_{i}(t)$ would give the shape of p(t), but it would excessively weight events that happened when the predictor stock price was high. So we will divide each $h_{i}(t)$ by the average value during the time period before the spike.\n",
    "\n",
    "## The Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7611 entries, 0 to 7610\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Symbol      7611 non-null   object \n",
      " 1   Name        7611 non-null   object \n",
      " 2   Last Sale   7611 non-null   object \n",
      " 3   Net Change  7611 non-null   float64\n",
      " 4   % Change    7598 non-null   object \n",
      " 5   Market Cap  6987 non-null   float64\n",
      " 6   Country     6951 non-null   object \n",
      " 7   IPO Year    4341 non-null   float64\n",
      " 8   Volume      7611 non-null   int64  \n",
      " 9   Sector      5502 non-null   object \n",
      " 10  Industry    5501 non-null   object \n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 654.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import leading_pattern.model as model\n",
    "import leading_pattern.by_yield as by_yield\n",
    "import leading_pattern.roc_auc as roc_auc\n",
    "import leading_pattern.plotting as plotting\n",
    "import my_utils.loaders as loaders\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "np.seterr(all='raise')  # make code stop on warnings\n",
    "\n",
    "# Two years of historical data. The first year is for training, the second year is for testing.\n",
    "CONFIG = {\n",
    "    'start_date': '20170103',\n",
    "    'split_date': '20171231',\n",
    "    'end_date': '20181231',\n",
    "    'spike_length': 2,\n",
    "    'spike_threshold_percent': 3.0,\n",
    "    'predictor_func_length': 7,\n",
    "    'use_diffs': True,\n",
    "    'price_field': 'Open',\n",
    "    'predictor_field': 'Open'\n",
    "}\n",
    "\n",
    "# We can get a CSV file of all the stocks in the NASDAQ from here: https://www.nasdaq.com/market-activity/stocks/screener\n",
    "# Lets read all the stocks into a pandas dataframe\n",
    "all_stocks = model.read_nasdaq_stock_list()\n",
    "\n",
    "print(all_stocks.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any list like this, there is bound to be some oddballs. Lets look at some summary statistics.\n",
    "\n",
    "But first lets fix \"Last Sale\", which is the price of the last sale. The values have a leading $, so pandas treats it as a string. Lets convert it to a float. Lets also add a column that is the \"Last Sale\" x \"Volume\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last Sale</th>\n",
       "      <th>Net Change</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>IPO Year</th>\n",
       "      <th>Volume</th>\n",
       "      <th>dollar_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7611.000000</td>\n",
       "      <td>7611.000000</td>\n",
       "      <td>6.987000e+03</td>\n",
       "      <td>4341.000000</td>\n",
       "      <td>7.611000e+03</td>\n",
       "      <td>7.611000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>97.379191</td>\n",
       "      <td>0.938466</td>\n",
       "      <td>8.862684e+09</td>\n",
       "      <td>2013.480534</td>\n",
       "      <td>1.039174e+06</td>\n",
       "      <td>4.744837e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4687.640393</td>\n",
       "      <td>33.756277</td>\n",
       "      <td>5.822363e+10</td>\n",
       "      <td>9.247937</td>\n",
       "      <td>7.483577e+06</td>\n",
       "      <td>3.623841e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.052000</td>\n",
       "      <td>-11.470000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.250000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.475000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.752196e+07</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2.209850e+04</td>\n",
       "      <td>2.550942e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.240000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>5.066325e+08</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>1.299790e+05</td>\n",
       "      <td>1.733926e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.450000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>3.174283e+09</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>5.591820e+05</td>\n",
       "      <td>1.470607e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>408840.000000</td>\n",
       "      <td>2939.000000</td>\n",
       "      <td>2.328752e+12</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>4.955231e+08</td>\n",
       "      <td>2.069273e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Last Sale   Net Change    Market Cap     IPO Year        Volume  \\\n",
       "count    7611.000000  7611.000000  6.987000e+03  4341.000000  7.611000e+03   \n",
       "mean       97.379191     0.938466  8.862684e+09  2013.480534  1.039174e+06   \n",
       "std      4687.640393    33.756277  5.822363e+10     9.247937  7.483577e+06   \n",
       "min         0.052000   -11.470000  0.000000e+00  1972.000000  1.000000e+00   \n",
       "25%         9.475000     0.000000  9.752196e+07  2011.000000  2.209850e+04   \n",
       "50%        18.240000     0.090000  5.066325e+08  2017.000000  1.299790e+05   \n",
       "75%        36.450000     0.480000  3.174283e+09  2020.000000  5.591820e+05   \n",
       "max    408840.000000  2939.000000  2.328752e+12  2021.000000  4.955231e+08   \n",
       "\n",
       "         dollar_vol  \n",
       "count  7.611000e+03  \n",
       "mean   4.744837e+07  \n",
       "std    3.623841e+08  \n",
       "min    4.250000e-01  \n",
       "25%    2.550942e+05  \n",
       "50%    1.733926e+06  \n",
       "75%    1.470607e+07  \n",
       "max    2.069273e+10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stocks['Last Sale'] = all_stocks['Last Sale'].apply(lambda x: float(x.replace('$','')))\n",
    "all_stocks['dollar_vol'] = all_stocks['Last Sale']* all_stocks['Volume']\n",
    "all_stocks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last-Sale is the price of the last trade. Look at that max! Almost a half million. IPO Year is a little odd because Yahoo reader has convert the int into a float. We will exclude stocks that had an IPO Year after our analysis start year minus one year.\n",
    "\n",
    "Volume seems like might be a reasonable field for removing outliers. To speed up the exploratory analysis, lets limit the number of predictor stocks to a random sample of 10, whose volume is above the 0.1 quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 251 entries, 2017-01-03 to 2017-12-29\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   High       251 non-null    float64\n",
      " 1   Low        251 non-null    float64\n",
      " 2   Open       251 non-null    float64\n",
      " 3   Close      251 non-null    float64\n",
      " 4   Volume     251 non-null    float64\n",
      " 5   Adj Close  251 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "analysis_start_year = float(CONFIG['start_date'][0:4])\n",
    "symbols = model.select_random_stock_symbols(all_stocks, 10, analysis_start_year, quantile=0.1, filter_field='Volume')\n",
    "\n",
    "# Use these from a previous random sample\n",
    "symbols = ['NPTN', 'MTL', 'AGR', 'PHD', 'ESS', 'CBOE', 'AGRO', 'FPF', 'CHMA', 'SEAS']\n",
    "\n",
    "data_sets = loaders.get_data_sets(symbols, CONFIG)\n",
    "\n",
    "# Lets select the target stock and look and it's columns\n",
    "target_stock = symbols[0]  # we will try to predict the first one\n",
    "print(data_sets['train'][target_stock].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 6 columns are primary data. It's not clear which to use for this project. One thought might be to use them all; maybe in a neural net? But there is a problem...\n",
    "\n",
    "## Water Everywhere, Nothing to Drink\n",
    "Although we have lots of data, we do not have enough of the right kind of data to use every tool in the machine learning toolbox. The big problem is that the stock market and the economy are constantly changing. It's not clear if the patterns in the data from 5 years ago are still relevant data now.\n",
    "\n",
    "For now, we are using historical data from the two years before covid.\n",
    "\n",
    "Also, we chose to limit our analysis to the daily opening price. While we could get more data by trying to get hourly data, that was not readily available. Moreover, when the time interval between data points is small enough we might run into other problems. Stock prices are set by trades and trades are discrete causing additional noise. And for a somewhat smooth continuous function, nearby values are highly correlated.\n",
    "\n",
    "## Back to the Historical Stock Data\n",
    "Lets look at the spikes in the target stock in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf48ff3e7f124dc5a8f96a5de83b1280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_sets['train'][target_stock]\n",
    "model.find_spikes(df, CONFIG)  # Find spikes in target stock and a column 'spikes' to dataframe.\n",
    "plotting.plot_spikes(df, CONFIG['price_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With 28 spikes, it looks like this target stock has enough volatility to train on. Lets load snippets of historical data in each predictor stock before each spike in the target stock. Note, we are also using the target stock as a predictor of its self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bd467dcce5443ca2205f4cb23323e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONFIG['use_diffs'] = False  # the reason for this will be clear in a moment\n",
    "predictor_func_by_stock = model.get_predictor_func_by_stock(data_sets['train'], target_stock, symbols, CONFIG)\n",
    "plotting.plot_predictor_functions(symbols, target_stock, predictor_func_by_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is surpising that these are so similar. We checked; it's not a bug. Although each predictor function is created from a different stock, they all are created from the same time points. What we are seeing is that all these stocks follow a similar trend. Below are the predictor functions derived from the same number of spikes at random times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d241fb2f2441ebabe334da39650030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make spikes at random times\n",
    "df = data_sets['train'][target_stock]\n",
    "spikes = df.spikes.copy()  # save so we can restore for further analysis\n",
    "spike_count = df['spikes'][df['spikes']].shape[0]\n",
    "random_spikes = np.full_like(df.spikes, False)\n",
    "indices = np.random.randint(0, df['spikes'].shape[0] - 1, spike_count)\n",
    "random_spikes[indices] = True\n",
    "df.spikes = random_spikes\n",
    "\n",
    "predictor_func_by_stock = model.get_predictor_func_by_stock(data_sets['train'], target_stock, symbols, CONFIG)\n",
    "plotting.plot_predictor_functions(symbols, target_stock, predictor_func_by_stock)\n",
    "df.spikes = spikes  # restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these predictor functions are similar as well, implying that the shape of the curves is mostly determined by the times of the spikes, not a pattern unique to each predictor stock.\n",
    " \n",
    "Our hypothesis is that over a time span of a few days, market information ripples through the predictor stocks before the spike in the target stock. To look for these ripples we should de-trend the data by taking the difference of successive time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a6469783914f698ad392f103c2adb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONFIG['use_diffs'] = True\n",
    "predictor_func_by_stock = model.get_predictor_func_by_stock(data_sets['train'], target_stock, symbols, CONFIG)\n",
    "plotting.plot_predictor_functions(symbols, target_stock, predictor_func_by_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the predictor functions look different for each predictor, which is what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Performance of Each Predictor\n",
    "The objective of this strategy is not neccessarily to make correct predictions. Rather it is to make money. Lets try running the predictors and look at the price changes after each positive prediction (true positive and false positive) as a function of threshold of $r^2$\n",
    "\n",
    "One way to do this is to just add up all the price changes to get the total yield. The problem with that approach is it combines the change from each trade with the number of trades. To remove the effect of trading frequency, we will calculate the expected-mean-percent-change.\n",
    "\n",
    "To calculate expected-mean-percent-change, we start by calculating all the percent changes in the test data. If the predictor did not predict the price then it would be like randomly sampling from these changes (null hypothesis). Next we set the $r^2$ threshold to predict when to buy which creates another sample of the percent changes. We use the Kolmogorov-Smirnov to calculate the probability (p) that our sample is the same as the random sample. Then we calculate expected mean percent change (mpc):\n",
    "\n",
    "$expected\\ mpc = p * mpc_{null}+(1-p)*mpc_{predictor}$\n",
    "\n",
    "$\\sigma_{empc}=\\sqrt{p^2*\\sigma_{null}^2+(1-p)^2*\\sigma_{predictor}^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7048020f51a94d638c97f46c65ee1d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_by_predictor = by_yield.calc_performance_by_threshold_for_predictors(target_stock, symbols, CONFIG, data_set_name='test')\n",
    "plotting.plot_calc_performance_scores_by_threshold_for_predictors(target_stock, df_by_predictor, CONFIG['predictor_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot low $r^2$ thresholds to show effectively random predictions. If a predictor worked, we would expect the optimal $r^2$ threshold to be higher.\n",
    "\n",
    "None of these predictors look great. This is not too surprising; most stocks are not strong predictors of other stocks, much less 10 randomly selected stocks. Further, the target stock itself might not be predictable using this strategy.\n",
    "\n",
    "Just for fun, we can run the code using a different data frame column without having to change the code. Lets try \"Volume\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb95fd598b34ed59890255369764e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONFIG['predictor_field'] = 'Volume'\n",
    "df_by_predictor = by_yield.calc_performance_by_threshold_for_predictors(target_stock, symbols, CONFIG, data_set_name='test')\n",
    "plotting.plot_calc_performance_scores_by_threshold_for_predictors(target_stock, df_by_predictor, CONFIG['predictor_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only calculate performance for $r^2$ thresholds where there are 5 or more predictions. That is why the threshold range is is truncated for some predictors.\n",
    "\n",
    "The only predictor that might be worth looking into is PHD.\n",
    "\n",
    "## Back to Reality\n",
    "In many machine learning projects, there is plenty of data and the system being modelled is stationary. This means that the data can be split into \"train\", \"dev\" and \"test\". We could use \"train\" to estimate the predictor functions. \"dev\" to pick the threshold and the predictor stocks. Then use \"test\" to see how well this works.\n",
    "\n",
    "Unfortunately our data is limited and not necessarily stationary. We will have to cut corners. To start, lets look at mean-percent-change on the training data. This is cheating since we are running the predictor functions on the data that generated them. But it is useful cheating because it gives us an upper bound on how well the strategy could work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e9e89ca6044609b2bc8d9ffd0aace2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONFIG['predictor_field'] = 'Open'\n",
    "df_by_predictor = by_yield.calc_performance_by_threshold_for_predictors(target_stock, symbols, CONFIG, data_set_name='train')\n",
    "plotting.plot_calc_performance_scores_by_threshold_for_predictors(target_stock, df_by_predictor, CONFIG['predictor_field'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to select predictor stocks based on training data, we might conclude that SEAS is a good predictor. Yet as we saw above, \"test\" shows that it is not a predictor at all.\n",
    "\n",
    "We ran all predictors and sorted the results based on the yield in the \"training\" data. The results with the 10 highest expected-mean-percent-change are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predictor      train      test\n",
      "0       NLOK  11.592130  0.020632\n",
      "1       PATK  11.134891  2.982296\n",
      "2       SLNO  10.975609  0.547020\n",
      "3       BIMI  10.246582  0.873495\n",
      "4        LCI  10.036559 -0.091898\n",
      "5       AGNC  10.004727  0.397897\n",
      "6       SPNS   9.921449  1.048367\n",
      "7       CCEP   9.914787  0.639683\n",
      "8        PGC   9.406757  1.599395\n",
      "9        ORA   8.831343  2.030144\n",
      "10       AXS   8.787117 -0.368236\n"
     ]
    }
   ],
   "source": [
    "df = by_yield.compare_train_test(target_stock, CONFIG)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently the training data is not a very good predictor of the test data. Maybe AXS is a good predictor, but if we are only looking at the training data performance, how would we select that and not LCI? There are avenues to explore. Maybe add a \"dev\" set. Maybe look at how similar the data snippets are that are used to create the predictor func.\n",
    "\n",
    "But there is another thing to consider. Our pragmatic use of mean-percent-change combines the strategy performance with the target stock prices. [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a way to look at the strategy in isolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c952976608424932bb827c5219b3fa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_set_name = 'test'\n",
    "predictor_func_by_stock = model.get_predictor_func_by_stock(data_sets['train'], target_stock, symbols, CONFIG)\n",
    "auc_df_by_predictor = {}\n",
    "for predictor, p_func in predictor_func_by_stock.items():\n",
    "    model.calc_r_squared_for_list(data_sets[data_set_name], predictor_func_by_stock, CONFIG)\n",
    "    auc_df_by_predictor[predictor] = \\\n",
    "        roc_auc.calc_fpr_tpr(data_sets[data_set_name][target_stock], data_sets[data_set_name][predictor], CONFIG)\n",
    "plotting.plot_roc_panel(auc_df_by_predictor, target_stock, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these look very promising.\n",
    "\n",
    "We ran this analysis for all predictors and sorted by the area-under-the-curve (AUC) for the training data. Once again, the training performance is not a good predictor of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  predictor     train      test\n",
      "0       PEI  0.633284  0.512862\n",
      "1      ALJJ  0.629863  0.515574\n",
      "2      AMOT  0.623851  0.501561\n",
      "3       UUU  0.617693  0.546721\n",
      "4       GLP  0.617644  0.519231\n",
      "5       HMY  0.614125  0.516806\n",
      "6      LMFA  0.613978  0.508670\n",
      "7      USDP  0.612512  0.522559\n",
      "8      NOMD  0.611877  0.558720\n",
      "9       CVU  0.610411  0.527120\n"
     ]
    }
   ],
   "source": [
    "df = roc_auc.summarize_auc(target_stock, CONFIG, max_rows=10)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like NOMD is the best of the not-very-good. Lets say we somehow selected that as a predictor. How might it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   threshold       fpr       tpr       ppv       npv\n",
      "0       0.95  0.000000  0.000000  0.000000  0.701245\n",
      "1       0.85  0.000000  0.013889  1.000000  0.704167\n",
      "2       0.75  0.023669  0.027778  0.333333  0.702128\n",
      "3       0.65  0.041420  0.027778  0.222222  0.698276\n",
      "4       0.55  0.053254  0.097222  0.437500  0.711111\n",
      "5       0.45  0.088757  0.180556  0.464286  0.723005\n",
      "6       0.35  0.159763  0.277778  0.425532  0.731959\n",
      "7       0.25  0.213018  0.333333  0.400000  0.734807\n",
      "8       0.15  0.343195  0.430556  0.348315  0.730263\n"
     ]
    }
   ],
   "source": [
    "predictor = 'NOMD'\n",
    "data_sets2 = loaders.get_data_sets([target_stock, predictor], CONFIG)\n",
    "predictor_func_by_stock2 = model.get_predictor_func_by_stock(data_sets2['train'], target_stock, [predictor], CONFIG)\n",
    "model.calc_r_squared_for_list(data_sets2['test'], predictor_func_by_stock2, CONFIG)\n",
    "auc_df = roc_auc.calc_fpr_tpr(data_sets2['test'][target_stock], data_sets2['test'][predictor], CONFIG)\n",
    "print(auc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear what threshold is optimal. For sake of argument, lets say 0.55 is best and that tpr=10% with fpr=5%. For NPTN, there a about 30 real spikes in a year. That means we would get 3 true positives and 1.5 false positives. Note, if we select the next higher threshold, the fpr is about twice the tpr and the tpr drops dramatically. Selecting a threshold is clearly important.\n",
    "\n",
    "To double our money, we need to have about 25 true positives in a year. So we would need about 8 stocks like NPTN. That might be possible if we could find a robust method for selecting predictors and thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If we wanted to continue with this strategy, then next step would be to find a way to pick the predictors and thresholds using the shortest time span possible. This could involve including a split just for that. Maybe 10 months for training, 3 months for dev and 3 months for test. Maybe interleave the splits.\n",
    "\n",
    "Another option would be to look at how the predictor function converges. The predictor function is made by averaging many snippets. If many of the snippets looked similar, then the average might be meaningful. Whereas if each snippet looked completely different, we would still get an average, but it would just be an average of noise.\n",
    "\n",
    "Or maybe a completely different strategy..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}