{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stock Trading Strategy Based on Neural Net Time Series Analysis\n",
    "\n",
    "For background, please see our previous notebook: lp_notebook.ipynb.\n",
    "\n",
    "The basic idea is to predict the price of a target stock based on historical values (price, volume, etc..) of a set of predictor stocks; an event happens, it ripples through predictors then to the target. In our previous models we found this is difficult... maybe even impossible. Maybe we will have better luck with neural networks (NN).\n",
    "\n",
    "One possible benefit of NN is they make it easy to combine predictors. Previously we looked for individual stocks as predictors with the thought that we would find a way to combine them later. With the previous models it is even possible we could have two or more predictors that by themselves could not exceed a threshold, but together they would. With NN we would find such a predictors.\n",
    "\n",
    "## Unbalanced Classes\n",
    "In the previous analysis, we looked for spikes in the target that lasted 2 days and exceeded a 3% gain. That approach gave us about 10-20 events/year for an average stock. Using a NN classifier on data like that would fail because the algorithm would be right 90% of the time just by predicting \"no price increase\" everyday. To get around that issue, we tried using a NN regression model to predict the stock price for every day.\n",
    "\n",
    "### The Regression Model\n",
    "In a sense the regression model worked really well, which is not all that surprising given that the past prices of the target stock were used for prediction, and the prices of a stock do not change very much day-to-day. However, as a stock buying strategy the results were almost useless because the model was usually a day late.\n",
    "\n",
    "### Classification Model\n",
    "Back to a classification NN. One way to balance the classes is to just decrease the price threshold for getting into the positive class. The problem with that approach is it is not consistent with our hypothesis. It is likely that small price increases are not related to any predictor. Adding them as positive samples is just adding noise.\n",
    "\n",
    "Furthermore, when we were making synthetic data to test the model, we discovered that our model is not designed to handle price increases that happen over multiple days. To accommodate this, we changed our label maker. A label is positive if:\n",
    "\n",
    "    1. (price[today] - price[yesterday]) / price[yesterday]  < threshold\n",
    "    2. (price[tomorrow] - price[today]) / price[today]  >= threshold\n",
    "    3. today is at least a convolution kernel width after the last positive label\n",
    "\n",
    "Of course, this does not resolve the unbalanced classes. There are multiple techniques for dealing with unbalanced classes. Throwing away data seems wrong. So whenever possible we prefer to synthesize data to balance the classes. In this case, it is not clear how to do that. So we will solve the problem by under-sampling the majority class.\n",
    "\n",
    "## Train, Validate and Test\n",
    "This data is not stationary. So we should intermix the training and validation samples. This is somewhat tricky because each sample extends over a few time points and we do not want the training and validation samples to contain any of the same time points.\n",
    "\n",
    "In production, we will only be predicting the next day and we will be doing that sequentially. So the test data is not intermixed.\n",
    "\n",
    "## Needles in a Haystack\n",
    "There are 5000 stocks in the NASDAQ. The number of possible combinations of predictors for each stock is enormous. The ideal model would allow us to test many predictors at once. This is similar to image classification, where there is an enormous number of pixels. Convolution models work well for this. We will give it a try in our domain.\n",
    "\n",
    "## 1D Convolution Model\n",
    "Our hypothesis is that in the few days before the target stock price goes up, there will be a signal in the predictors. Thus each sample is a 2D (conv_window x n_predictors) tensor. Using these samples, each 1D convolution filter has (conv_window x n_predictors) weights. Every predictor has it's own kernel (column) in this filter. \n",
    "\n",
    "After the convolution layer, there needs to be at least one dense layer with the number of inputs equal to the number of filters. If we let:\n",
    "\n",
    "\n",
    "p = number predictors\n",
    "\n",
    "f = number 1D convolution filters\n",
    "\n",
    "k = number time points per sample\n",
    "\n",
    "\n",
    "number of weights = $p*f*k + f \\approx p * f * k$\n",
    "\n",
    "It seems that approach gives too many degrees of freedom, which will exacerbate over-fitting and increase training time. Our intuition is that we do not need a unique kernel for each predictor for each filter. Rather a small number of kernels, maybe a max of 10, would work for any number of predictors.\n",
    "\n",
    "To accomplish this, we changed the shape of each sample by stacking the conv_window of points for each predictor. Now each 1D convolution filter has only one kernel. To prevent mixing of data between predictors in this stacked configuration, we set the convolution stride to conv_window. The number of outputs from each filter is now equal to the number of predictors. And the dense layer now has f * p inputs. The number of weights is:\n",
    "\n",
    "number of weights = $k*f + f*p \\approx p * f$\n",
    "\n",
    "We are using k=3, so this modified 1D convolution has a factor of 3 fewer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n predictors: 10\n",
      "n convolution filters: 4\n",
      "['CUEN', 'VERB', 'VRME', 'NMTR', 'AEYE', 'PLAG', 'BLNK', 'ANY', 'ELOX', 'CREX']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nn.nn as nn\n",
    "import nn.nn2 as nn2\n",
    "from my_utils.volatility import load_volatility\n",
    "from window_generators import IntermixedWindowGenerator\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "\n",
    "config = {\n",
    "    'start_date': '20170103',\n",
    "    'end_date': '20181231',\n",
    "    'price_field': 'Open',\n",
    "    'predictor_field': 'Open'\n",
    "}\n",
    "splits = [0.5, 0.75]\n",
    "CONV_WIDTH = 3\n",
    "\n",
    "# Load the most volatile stocks as predictors\n",
    "results_dir = nn2.get_results_dir(config)\n",
    "\n",
    "volatility = load_volatility(results_dir, config)\n",
    "target = 'PLAG'\n",
    "predictors = [x[0] for x in volatility[0:10]]\n",
    "dataframe = nn.load_data(target, predictors, config)\n",
    "n_predictors = dataframe.shape[1]\n",
    "n_filters = max(min(10, int(n_predictors / 4)), 4)\n",
    "print('n predictors: {}'.format(n_predictors))\n",
    "print('n convolution filters: {}'.format(n_filters))\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to label the data using the method described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent positive: 9.362549800796813\n"
     ]
    }
   ],
   "source": [
    "# For this target we could not get more than 9% of the samples to be in the (+) class\n",
    "labels, threshold, frac_pos = nn2.make_labels(dataframe.target, CONV_WIDTH, frac_positive=0.09)\n",
    "print('Percent positive: {}'.format(100 * frac_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor data varies wildly from stock to stock, and over time. Our hypothesis is that we are looking for localized ripples in the predictor. This suggests that the data can be normalized by taking the difference between successive time points and dividing by the value at one of those time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "norm_df = nn2.diff_norm(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to make the data samples. As mentioned above, we intermixed the train and validation samples. To balance the data, we under-sampled the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "conv_window = IntermixedWindowGenerator(norm_df, labels, splits, CONV_WIDTH, balanced=True)\n",
    "model = nn2.limited_filters_conv_model(n_filters, CONV_WIDTH, n_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not obvious what loss function we should use. It is likely that not every price gain in the target is related to a predictor. So we expect a non-zero rate of false negatives, maybe as high as 50%. At the same time, false positives will either tie up our money on buys with meager gains or worse loose money. The way to avoid false positives is to set the threshold high. This will also increase false negatives. But what is the right rate of false negatives? And if we set the threshold too high, we will not make any buys; we might as well skip all this and just put our money in a stock index fund.\n",
    "\n",
    "For now, we will go with binary-cross-entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 2s 697ms/step - loss: 0.7843 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5789 - prc: 0.5764 - val_loss: 0.7968 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5877\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7832 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5718 - prc: 0.5584 - val_loss: 0.7961 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5618\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7822 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5598 - prc: 0.5439 - val_loss: 0.7953 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6201 - val_prc: 0.6112\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7814 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5455 - prc: 0.5364 - val_loss: 0.7943 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6299 - val_prc: 0.6483\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7806 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5502 - prc: 0.5241 - val_loss: 0.7934 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6104 - val_prc: 0.6228\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7796 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5586 - prc: 0.5268 - val_loss: 0.7928 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6071 - val_prc: 0.6214\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7789 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6065 - prc: 0.5656 - val_loss: 0.7923 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5844 - val_prc: 0.5852\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7781 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6005 - prc: 0.6135 - val_loss: 0.7918 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5682 - val_prc: 0.5652\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7774 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5981 - prc: 0.5981 - val_loss: 0.7912 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5942 - val_prc: 0.5792\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7766 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6172 - prc: 0.6089 - val_loss: 0.7906 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6006 - val_prc: 0.6304\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7757 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6148 - prc: 0.5853 - val_loss: 0.7899 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.6014\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7751 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5849 - prc: 0.5561 - val_loss: 0.7890 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6006 - val_prc: 0.6119\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7741 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5861 - prc: 0.5419 - val_loss: 0.7882 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6006 - val_prc: 0.6123\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7732 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5993 - prc: 0.5446 - val_loss: 0.7872 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6201 - val_prc: 0.6168\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7724 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5933 - prc: 0.5517 - val_loss: 0.7862 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5779 - val_prc: 0.5729\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7715 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6376 - prc: 0.6188 - val_loss: 0.7852 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5812 - val_prc: 0.6102\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7705 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6196 - prc: 0.5982 - val_loss: 0.7842 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.6077\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7695 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5861 - prc: 0.5727 - val_loss: 0.7832 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5747 - val_prc: 0.5916\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7686 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5694 - prc: 0.5439 - val_loss: 0.7821 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6234 - val_prc: 0.6162\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7677 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6041 - prc: 0.5689 - val_loss: 0.7809 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6104 - val_prc: 0.5884\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7665 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6352 - prc: 0.6378 - val_loss: 0.7798 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5714 - val_prc: 0.5725\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7655 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6352 - prc: 0.6033 - val_loss: 0.7787 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.6077\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7643 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6124 - prc: 0.5938 - val_loss: 0.7775 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6006 - val_prc: 0.6082\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7635 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6065 - prc: 0.6026 - val_loss: 0.7762 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5974 - val_prc: 0.5802\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7622 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6160 - prc: 0.6175 - val_loss: 0.7749 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5714 - val_prc: 0.5725\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7611 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6280 - prc: 0.6201 - val_loss: 0.7736 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.5747\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7600 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6220 - prc: 0.6021 - val_loss: 0.7722 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6234 - val_prc: 0.6409\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7588 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6136 - prc: 0.6049 - val_loss: 0.7708 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5812 - val_prc: 0.5738\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7577 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6196 - prc: 0.6063 - val_loss: 0.7695 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5965\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7566 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6316 - prc: 0.6172 - val_loss: 0.7682 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5768\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7553 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6292 - prc: 0.6213 - val_loss: 0.7669 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5779 - val_prc: 0.5811\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7543 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6160 - prc: 0.6079 - val_loss: 0.7655 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5804\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7530 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6089 - prc: 0.6013 - val_loss: 0.7642 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.5862\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7518 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6196 - prc: 0.6172 - val_loss: 0.7629 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.5829\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7506 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6017 - prc: 0.6076 - val_loss: 0.7616 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5942 - val_prc: 0.5772\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7493 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6208 - prc: 0.6308 - val_loss: 0.7603 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5772\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7480 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6053 - prc: 0.5990 - val_loss: 0.7589 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.5877\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7467 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6065 - prc: 0.5983 - val_loss: 0.7575 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5844 - val_prc: 0.5784\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7453 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6340 - prc: 0.6261 - val_loss: 0.7560 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.6039 - val_prc: 0.5881\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7440 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6100 - prc: 0.6213 - val_loss: 0.7546 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.5858\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7426 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6268 - prc: 0.6219 - val_loss: 0.7531 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5812 - val_prc: 0.5757\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7412 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6089 - prc: 0.6059 - val_loss: 0.7516 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5877 - val_prc: 0.5737\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7398 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6256 - prc: 0.6285 - val_loss: 0.7502 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5584 - val_prc: 0.5676\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7385 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6124 - prc: 0.6135 - val_loss: 0.7488 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5682 - val_prc: 0.5716\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7368 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6292 - prc: 0.6360 - val_loss: 0.7473 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5844 - val_prc: 0.5553\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7354 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6471 - prc: 0.6483 - val_loss: 0.7456 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5552 - val_prc: 0.5678\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7341 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.5933 - prc: 0.6059 - val_loss: 0.7437 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5909 - val_prc: 0.5800\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7323 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6364 - prc: 0.6331 - val_loss: 0.7419 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5649 - val_prc: 0.5709\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7310 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6172 - prc: 0.6206 - val_loss: 0.7401 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5974 - val_prc: 0.5911\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7293 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6304 - prc: 0.6308 - val_loss: 0.7384 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5519 - val_prc: 0.5419\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7274 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6316 - prc: 0.6271 - val_loss: 0.7368 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5844 - val_prc: 0.5841\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7263 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6208 - prc: 0.6272 - val_loss: 0.7350 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5519 - val_prc: 0.5419\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7241 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6388 - prc: 0.6398 - val_loss: 0.7333 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5812 - val_prc: 0.5796\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7229 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6256 - prc: 0.6307 - val_loss: 0.7315 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5519 - val_prc: 0.5419\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7211 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6280 - prc: 0.6287 - val_loss: 0.7297 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5714 - val_prc: 0.5767\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7195 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6280 - prc: 0.6312 - val_loss: 0.7279 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5649 - val_prc: 0.5471\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7179 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6364 - prc: 0.6243 - val_loss: 0.7262 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5682 - val_prc: 0.5540\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7163 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6376 - prc: 0.6429 - val_loss: 0.7247 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5649 - val_prc: 0.5571\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7144 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6388 - prc: 0.6250 - val_loss: 0.7232 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5390 - val_prc: 0.5437\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7130 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6411 - prc: 0.6477 - val_loss: 0.7216 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5649 - val_prc: 0.5540\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7114 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6543 - prc: 0.6425 - val_loss: 0.7201 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5455 - val_prc: 0.5439\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7100 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6459 - prc: 0.6437 - val_loss: 0.7186 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5390 - val_prc: 0.5406\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7082 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6663 - prc: 0.6552 - val_loss: 0.7173 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5390 - val_prc: 0.5438\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7067 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6687 - prc: 0.6703 - val_loss: 0.7160 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5357 - val_prc: 0.5278\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7052 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6746 - prc: 0.6624 - val_loss: 0.7146 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5455 - val_prc: 0.5465\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7044 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6663 - prc: 0.6723 - val_loss: 0.7135 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5455 - val_prc: 0.5559\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7023 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6770 - prc: 0.6747 - val_loss: 0.7128 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5357 - val_prc: 0.5408\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7015 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6782 - prc: 0.6787 - val_loss: 0.7120 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5357 - val_prc: 0.5395\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7000 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6902 - prc: 0.6840 - val_loss: 0.7112 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5357 - val_prc: 0.5518\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6987 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6878 - prc: 0.6963 - val_loss: 0.7102 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5162 - val_prc: 0.5264\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6978 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6830 - prc: 0.6770 - val_loss: 0.7092 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5357 - val_prc: 0.5370\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6963 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6854 - prc: 0.6956 - val_loss: 0.7083 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5162 - val_prc: 0.5374\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6951 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6962 - prc: 0.6927 - val_loss: 0.7075 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5390 - val_prc: 0.5426\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6939 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.6914 - prc: 0.7017 - val_loss: 0.7067 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5195 - val_prc: 0.5326\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6927 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.7022 - prc: 0.7048 - val_loss: 0.7059 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5292 - val_prc: 0.5417\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6915 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.7093 - prc: 0.7145 - val_loss: 0.7052 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5357 - val_prc: 0.5418\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6905 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.7022 - prc: 0.7166 - val_loss: 0.7044 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5260 - val_prc: 0.5313\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6892 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.7165 - prc: 0.7257 - val_loss: 0.7036 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5162 - val_prc: 0.5384\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6880 - tp: 19.0000 - fp: 22.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - precision: 0.4634 - recall: 1.0000 - auc: 0.7165 - prc: 0.7250 - val_loss: 0.7028 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5292 - val_prc: 0.5331\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6867 - tp: 19.0000 - fp: 21.0000 - tn: 1.0000 - fn: 0.0000e+00 - precision: 0.4750 - recall: 1.0000 - auc: 0.7309 - prc: 0.7485 - val_loss: 0.7021 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.4968 - val_prc: 0.5211\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6855 - tp: 19.0000 - fp: 21.0000 - tn: 1.0000 - fn: 0.0000e+00 - precision: 0.4750 - recall: 1.0000 - auc: 0.7285 - prc: 0.7415 - val_loss: 0.7013 - val_tp: 11.0000 - val_fp: 14.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_precision: 0.4400 - val_recall: 1.0000 - val_auc: 0.5130 - val_prc: 0.5149\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6840 - tp: 19.0000 - fp: 21.0000 - tn: 1.0000 - fn: 0.0000e+00 - precision: 0.4750 - recall: 1.0000 - auc: 0.7321 - prc: 0.7471 - val_loss: 0.7005 - val_tp: 10.0000 - val_fp: 13.0000 - val_tn: 1.0000 - val_fn: 1.0000 - val_precision: 0.4348 - val_recall: 0.9091 - val_auc: 0.4870 - val_prc: 0.5124\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6828 - tp: 19.0000 - fp: 20.0000 - tn: 2.0000 - fn: 0.0000e+00 - precision: 0.4872 - recall: 1.0000 - auc: 0.7249 - prc: 0.7438 - val_loss: 0.6996 - val_tp: 10.0000 - val_fp: 13.0000 - val_tn: 1.0000 - val_fn: 1.0000 - val_precision: 0.4348 - val_recall: 0.9091 - val_auc: 0.5097 - val_prc: 0.5364\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6817 - tp: 19.0000 - fp: 20.0000 - tn: 2.0000 - fn: 0.0000e+00 - precision: 0.4872 - recall: 1.0000 - auc: 0.7464 - prc: 0.7727 - val_loss: 0.6989 - val_tp: 10.0000 - val_fp: 13.0000 - val_tn: 1.0000 - val_fn: 1.0000 - val_precision: 0.4348 - val_recall: 0.9091 - val_auc: 0.4870 - val_prc: 0.5137\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6803 - tp: 18.0000 - fp: 18.0000 - tn: 4.0000 - fn: 1.0000 - precision: 0.5000 - recall: 0.9474 - auc: 0.7440 - prc: 0.7709 - val_loss: 0.6984 - val_tp: 8.0000 - val_fp: 13.0000 - val_tn: 1.0000 - val_fn: 3.0000 - val_precision: 0.3810 - val_recall: 0.7273 - val_auc: 0.5000 - val_prc: 0.5223\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6794 - tp: 18.0000 - fp: 18.0000 - tn: 4.0000 - fn: 1.0000 - precision: 0.5000 - recall: 0.9474 - auc: 0.7404 - prc: 0.7715 - val_loss: 0.6980 - val_tp: 8.0000 - val_fp: 12.0000 - val_tn: 2.0000 - val_fn: 3.0000 - val_precision: 0.4000 - val_recall: 0.7273 - val_auc: 0.4740 - val_prc: 0.4951\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6779 - tp: 18.0000 - fp: 16.0000 - tn: 6.0000 - fn: 1.0000 - precision: 0.5294 - recall: 0.9474 - auc: 0.7524 - prc: 0.7859 - val_loss: 0.6979 - val_tp: 8.0000 - val_fp: 12.0000 - val_tn: 2.0000 - val_fn: 3.0000 - val_precision: 0.4000 - val_recall: 0.7273 - val_auc: 0.4805 - val_prc: 0.4968\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6771 - tp: 18.0000 - fp: 16.0000 - tn: 6.0000 - fn: 1.0000 - precision: 0.5294 - recall: 0.9474 - auc: 0.7691 - prc: 0.8049 - val_loss: 0.6978 - val_tp: 8.0000 - val_fp: 12.0000 - val_tn: 2.0000 - val_fn: 3.0000 - val_precision: 0.4000 - val_recall: 0.7273 - val_auc: 0.4838 - val_prc: 0.5005\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6761 - tp: 18.0000 - fp: 16.0000 - tn: 6.0000 - fn: 1.0000 - precision: 0.5294 - recall: 0.9474 - auc: 0.7739 - prc: 0.8012 - val_loss: 0.6979 - val_tp: 8.0000 - val_fp: 12.0000 - val_tn: 2.0000 - val_fn: 3.0000 - val_precision: 0.4000 - val_recall: 0.7273 - val_auc: 0.4773 - val_prc: 0.4989\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6751 - tp: 18.0000 - fp: 16.0000 - tn: 6.0000 - fn: 1.0000 - precision: 0.5294 - recall: 0.9474 - auc: 0.7667 - prc: 0.7995 - val_loss: 0.6979 - val_tp: 7.0000 - val_fp: 11.0000 - val_tn: 3.0000 - val_fn: 4.0000 - val_precision: 0.3889 - val_recall: 0.6364 - val_auc: 0.4773 - val_prc: 0.5022\n"
     ]
    }
   ],
   "source": [
    "history = nn.compile_and_fit_classifier(model, conv_window, patience=2, max_epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 10, 3)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 30, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1, 10, 4)          16        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.7834928035736084\n",
      "Val   AUC: 0.47727274894714355\n"
     ]
    }
   ],
   "source": [
    "auc_train = model.evaluate(x=conv_window.train[0], y=conv_window.train[1], verbose=0)[7]\n",
    "auc_val = model.evaluate(x=conv_window.val[0], y=conv_window.val[1], verbose=0)[7]\n",
    "print('Train AUC: {}'.format(auc_train))\n",
    "print('Val   AUC: {}'.format(auc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the ROC AUC is for training is much higher than validation, indicating over-fitting.\n",
    "\n",
    "This is confirmed by the plot of the loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166de49c5b3347de8257ed54ada8e09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with over-fitting, the validation ROC AUC is near enough to 0.5 to indicate that these predictors are not predictors for this target. Which brings us back to the need to efficiently search the space of predictors.\n",
    "\n",
    "## Efficiently Looking for Predictors\n",
    "We can search the predictor space faster if we can test lots of predictors at once. How many should that be?\n",
    "\n",
    "To answer that question, we wrote some code to modify a predictor by adding a kernel to the predictor exactly before each positive class in the labels. When we ran the modified predictor along with the target through our model we got a perfect fit of both the training and validation data.\n",
    "\n",
    "This continued when we add 9 other predictors that did not predict this target. With 99 others, we got an AUC in the high 0.90s. With 499 others AUC was around 0.8. So we probably could search in blocks of 500.\n",
    "\n",
    "## Running on Test Data\n",
    "At this point there is no need to run the model on the test data because we have not found a set of predictors that work on validation.\n",
    "\n",
    "When we find a set of predictors that work, we will update the model weights every week as we run it on the test data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}